{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc8cb9d",
   "metadata": {},
   "source": [
    "# Gradio UI & LLM Integration\n",
    "\n",
    "Note: Gradio uses user's current setting to choose the theme 'dark mode' or 'loght mode'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4e8e8",
   "metadata": {},
   "source": [
    "# Add gradio in requirements.txt or pyproject.toml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3eeafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pyproject.toml\n",
    "\n",
    "# [project]\n",
    "# name = \"llm-engineering\"\n",
    "# version = \"0.1.0\"\n",
    "# requires-python = \">=3.11\"\n",
    "# dependencies = [\n",
    "#     \"gradio>=5.47.2\",\n",
    "#     # .. other dependencies\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc72ccb",
   "metadata": {},
   "source": [
    "Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf642ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7dc098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurtion loaded successfully. \n",
      "TEST_API_KEY: DUMMY-API-, \n",
      "OLLAMA_NEW_BASE_URL: http://localhost:11434/v1\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "TEST_API_KEY = os.getenv(\"TEST_API_KEY\")\n",
    "OLLAMA_NEW_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\")\n",
    "GEMMA3_1B = os.getenv(\"GEMMA3_1B\")\n",
    "\n",
    "if (TEST_API_KEY or OLLAMA_NEW_BASE_URL or GEMMA3_1B) is None:\n",
    "    raise ValueError(\"TEST_API_KEY or OLLAMA_NEW_BASE_URL or GEMMA3_1B environment variable is not set.\")\n",
    "else:\n",
    "    print(f\"Configurtion loaded successfully. \\nTEST_API_KEY: {TEST_API_KEY[:10]}, \\nOLLAMA_NEW_BASE_URL: {OLLAMA_NEW_BASE_URL}\")\n",
    "\n",
    "# build client\n",
    "client = OpenAI(base_url=OLLAMA_NEW_BASE_URL, api_key=TEST_API_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f575eb",
   "metadata": {},
   "source": [
    "LLM API function to integrate with UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "149aecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that helps people find information.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize with user input\n",
    "PAYLOAD = []\n",
    "PAYLOAD.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3707132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the request from user and call the LLM \n",
    "def chat(input: str) -> str:\n",
    "    print(f\"info: chat functions is called. \\nInput: {input}\")\n",
    "    PAYLOAD.append({\"role\": \"user\", \"content\": input})\n",
    "    response = client.chat.completions.create(model=GEMMA3_1B,\n",
    "                                              messages=PAYLOAD, stream=False)\n",
    "    msg = response.choices[0].message.content\n",
    "    PAYLOAD.append({\"role\": \"assistant\", \"content\": msg})\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f435047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: chat functions is called. \n",
      "Input: Hello, This is vivek from India. Tell me a funny fact.\n",
      "\n",
      "Response: \n",
      "Okay, hereâ€™s a funny fact for you:\n",
      "\n",
      "**Bananas are berries, but strawberries aren't.** \n",
      "\n",
      "It's a surprisingly complicated berry classification! \n",
      "\n",
      "---\n",
      "\n",
      "Want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "resp = chat(\"Hello, This is vivek from India. Tell me a funny fact.\")\n",
    "\n",
    "print(f\"\\nResponse: \\n{resp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1724a2b",
   "metadata": {},
   "source": [
    "Create a Simple UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: chat functions is called. \n",
      "Input: Hi\n",
      "info: chat functions is called. \n",
      "Input: what is the date today ?\n",
      "info: chat functions is called. \n",
      "Input: who is the prime minister of india ?\n",
      "info: chat functions is called. \n",
      "Input: what is the famous chess problem in DSA. the best approach to solve it ?\n",
      "info: chat functions is called. \n",
      "Input: help me write a simple GUI in gradio to connect llm for chatting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_box = gr.Textbox(label=\"Ask me anything\", placeholder=\"Type your question here...\", type=\"text\", lines=3, max_lines=10, max_length=100, autofocus=True)\n",
    "output_box = gr.Textbox(label=\"Response\", placeholder=\"You haven't asked anything yet...\", type=\"text\", lines=10, autoscroll=False, show_copy_button=True)\n",
    "\n",
    "llm_chatting = gr.Interface(fn=chat, inputs=input_box, outputs=output_box, \n",
    "                    title=\"LLM Chat\", \n",
    "                    description=\"ChatOllama\", flagging_mode='never')\n",
    "\n",
    "# Launch the Gradio interface\n",
    "llm_chatting.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f974ea7",
   "metadata": {},
   "source": [
    "Stop Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fb6c521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "llm_chatting.close()\n",
    "\n",
    "# close all interfaces\n",
    "gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
