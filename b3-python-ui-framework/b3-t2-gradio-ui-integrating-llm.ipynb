{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc8cb9d",
   "metadata": {},
   "source": [
    "# Gradio UI & LLM Integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4e8e8",
   "metadata": {},
   "source": [
    "# Add gradio in requirements.txt or pyproject.toml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3eeafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pyproject.toml\n",
    "\n",
    "# [project]\n",
    "# name = \"llm-engineering\"\n",
    "# version = \"0.1.0\"\n",
    "# requires-python = \">=3.11\"\n",
    "# dependencies = [\n",
    "#     \"gradio>=5.47.2\",\n",
    "#     # .. other dependencies\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc72ccb",
   "metadata": {},
   "source": [
    "Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdf642ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e7dc098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded. \n",
      "BASE_URL: http://localhost:11434/v1, \n",
      "MODEL: llama3.2:3b \n",
      "API Key: DUMMY-API-\n"
     ]
    }
   ],
   "source": [
    "# Load .env variables securely\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration variables from environment\n",
    "TEST_API_KEY = os.getenv(\"TEST_API_KEY\")\n",
    "OLLAMA_API_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\") # e.g., http://localhost:11434/v1\n",
    "LLM_MODEL = os.getenv(\"LLAMA3_3B\") # The specific model to use\n",
    "\n",
    "# Basic check to ensure required configuration is present\n",
    "if (TEST_API_KEY or OLLAMA_API_BASE_URL or LLM_MODEL) is None:\n",
    "    raise ValueError(\"One or more required environment variables is not set.\")\n",
    "else:\n",
    "    print(f\"Config loaded. \\nBASE_URL: {OLLAMA_API_BASE_URL}, \\nMODEL: {LLM_MODEL} \\nAPI Key: {TEST_API_KEY[:10]}\")\n",
    "\n",
    "# Initialize the OpenAI client. base_url allows connection to local/non-OpenAI providers.\n",
    "client = OpenAI(base_url=OLLAMA_API_BASE_URL, api_key=TEST_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f575eb",
   "metadata": {},
   "source": [
    "LLM API function to integrate with UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "149aecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that helps people find information.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize with user input\n",
    "PAYLOAD = []\n",
    "PAYLOAD.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3707132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: chat functions is called. \n",
      "Input: is my name Rahul ?\n"
     ]
    }
   ],
   "source": [
    "# Take the request from user and call the LLM \n",
    "def chat(input: str) -> str:\n",
    "    print(f\"info: chat functions is called. \\nInput: {input}\")\n",
    "    PAYLOAD.append({\"role\": \"user\", \"content\": input})\n",
    "    response = client.chat.completions.create(model=LLM_MODEL,\n",
    "                                              messages=PAYLOAD, stream=False)\n",
    "    msg = response.choices[0].message.content\n",
    "    PAYLOAD.append({\"role\": \"assistant\", \"content\": msg})\n",
    "\n",
    "    print(f\"Prompt: \\n{PAYLOAD}\")\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f435047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: chat functions is called. \n",
      "Input: Hello, This is vivek from India. Tell me a funny fact.\n",
      "\n",
      "Response: \n",
      "Okay, here’s a funny fact for you:\n",
      "\n",
      "**Bananas are berries, but strawberries aren't.** \n",
      "\n",
      "It's a surprisingly complicated berry classification! \n",
      "\n",
      "---\n",
      "\n",
      "Want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "resp = chat(\"Hello, This is vivek from India. Tell me a funny fact.\")\n",
    "\n",
    "print(f\"\\nResponse: \\n{resp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1724a2b",
   "metadata": {},
   "source": [
    "Create a Simple UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: chat functions is called. \n",
      "Input: hi my name is vivek I am s/w eng.\n",
      "Prompt: \n",
      "[{'role': 'system', 'content': \"You are a helpful assistant that helps people find information.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\"}, {'role': 'user', 'content': 'hi my name is vivek I am s/w eng.'}, {'role': 'assistant', 'content': \"Hi Vivek! It's nice to meet you. Sorry, but I'm a large language model, I don't have personal interactions, so we can start fresh anytime.\\n\\nSo, you're a software engineer, right? What area of software engineering are you into (e.g., web development, mobile app development, AI/ML, etc.)? Or what's your current project or interest that you'd like to discuss?\"}, {'role': 'user', 'content': 'what do you kno about me ?'}, {'role': 'assistant', 'content': \"I don't know much about you personally. You introduced yourself as Vivek, a software engineer, but that's the extent of my knowledge so far.\\n\\nIf you feel comfortable sharing more, I'm here to listen. I can ask you questions or provide information on topics related to your profession if that would be helpful. Otherwise, we can keep our conversation light and just talk about general interests or topics. What would you like to do?\"}, {'role': 'user', 'content': 'is my name Rahul ?'}, {'role': 'assistant', 'content': 'Actually, when you first messaged me, you introduced yourself as \"Vivek\", not Rahul.'}, {'role': 'user', 'content': 'hi my name is vivek I am s/w eng.'}, {'role': 'assistant', 'content': \"Déjà vu! You've just repeated what you already told me earlier. My apologies for not remembering correctly earlier. Yes, you're right, your name is Vivek and you're a software engineer. Let's start fresh and continue the conversation from where we left off!\"}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_box = gr.Textbox(label=\"Ask me anything\", placeholder=\"Type your question here...\", type=\"text\", lines=3, max_lines=10, max_length=100, autofocus=True)\n",
    "output_box = gr.Textbox(label=\"Response\", placeholder=\"You haven't asked anything yet...\", type=\"text\", lines=10, autoscroll=False, show_copy_button=True)\n",
    "\n",
    "llm_chatting = gr.Interface(fn=chat, inputs=input_box, outputs=output_box, \n",
    "                    title=\"LLM Chat\", \n",
    "                    description=\"ChatOllama\", flagging_mode='never')\n",
    "\n",
    "# Launch the Gradio interface\n",
    "llm_chatting.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f974ea7",
   "metadata": {},
   "source": [
    "Stop Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fb6c521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "llm_chatting.close()\n",
    "\n",
    "# close all interfaces\n",
    "gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
