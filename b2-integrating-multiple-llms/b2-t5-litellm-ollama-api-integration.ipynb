{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9a5c14",
   "metadata": {},
   "source": [
    "# Litellm API\n",
    "* Integrate ollama LLMs running locally\n",
    "* https://docs.litellm.ai/docs/\n",
    "* https://docs.litellm.ai/docs/providers/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc7f5d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61bf7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#from litellm import completion\n",
    "import litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5260281",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca55dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully for Ollama. \n",
      "Base URL: http://localhost:11434 \n",
      "Model: ollama/llama3.2:3b\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OLLAMA_NEW_BASE_URL = os.getenv(\"OLLAMA_NEW_BASE_URL\")\n",
    "LLAMA3_3B_MODEL = f\"ollama/{os.getenv(\"LLAMA3_3B\")}\"\n",
    "\n",
    "if not OLLAMA_NEW_BASE_URL and LLAMA3_3B_MODEL:\n",
    "    print(\"One or more mandatory config missing.\")\n",
    "else:\n",
    "    print(f\"Config loaded successfully for Ollama. \\nBase URL: {OLLAMA_NEW_BASE_URL} \\nModel: {LLAMA3_3B_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8794d1",
   "metadata": {},
   "source": [
    "## Connect to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce969f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on debugging \n",
    "#litellm._turn_on_debug()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7438004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Raw Response:** \n",
      "ModelResponse(id='chatcmpl-941401da-7361-4816-a96e-797783b87a50', created=1762389242, model='ollama/llama3.2:3b', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Here\\'s one that\\'s \"punderful\":\\n\\nWhy couldn\\'t the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired! Get it?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage=Usage(completion_tokens=36, prompt_tokens=42, total_tokens=78, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "\n",
      "\n",
      "**Joke:** \n",
      "Here's one that's \"punderful\":\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired! Get it?\n"
     ]
    }
   ],
   "source": [
    "payload = [\n",
    "    {'role': 'system', 'content': 'You are a funny assistant.'},\n",
    "    {'role': 'user', 'content': 'tell me a joke!'}\n",
    "]\n",
    "\n",
    "response = litellm.completion(base_url=OLLAMA_NEW_BASE_URL, model= LLAMA3_3B_MODEL, messages=payload)\n",
    "\n",
    "print(f\"**Raw Response:** \\n{response}\\n\\n\")\n",
    "\n",
    "print(f\"**Joke:** \\n{response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d61c505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 42\n",
      "Output tokens: 36\n",
      "Total tokens: 78\n",
      "Hidden Params in response: {'custom_llm_provider': 'ollama', 'region_name': None, 'optional_params': {}, 'litellm_call_id': '2271f8d9-0dc1-4c43-844f-add355824d84', 'api_base': None, 'model_id': None, 'response_cost': 0.0, 'additional_headers': {}, 'litellm_model_name': 'ollama/llama3.2:3b'}\n",
      "Response Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Token details\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "print(f\"Hidden Params in response: {response._hidden_params}\")\n",
    "print(f\"Response Cost: {response._hidden_params['response_cost']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
