{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa025c7e",
   "metadata": {},
   "source": [
    "# Demonstrate a Pro-feature: Prompt Caching using litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eaa9885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "# Story file \n",
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "\n",
    "print(hamlet[loc:loc + 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6dadce",
   "metadata": {},
   "source": [
    "## Connect with local Ollama LLM\n",
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ffc5097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully for Ollama. \n",
      "Base URL: http://localhost:11434 \n",
      "Model: ollama/llama3.2:3b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import litellm\n",
    "from IPython.display import Markdown\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OLLAMA_NEW_BASE_URL = os.getenv(\"OLLAMA_NEW_BASE_URL\")\n",
    "LLAMA3_3B_MODEL = f\"ollama/{os.getenv(\"LLAMA3_3B\")}\"\n",
    "\n",
    "if not OLLAMA_NEW_BASE_URL and LLAMA3_3B_MODEL:\n",
    "    print(\"One or more mandatory config missing.\")\n",
    "else:\n",
    "    print(f\"Config loaded successfully for Ollama. \\nBase URL: {OLLAMA_NEW_BASE_URL} \\nModel: {LLAMA3_3B_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad03a0",
   "metadata": {},
   "source": [
    "# Build with lite llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f244238",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fd0671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: \n",
      "1. openrouter/minimax/minimax-m2:free \n",
      "2. openrouter/z-ai/glm-4.5-air:free\n"
     ]
    }
   ],
   "source": [
    "openrouter_prefix = \"openrouter/\"\n",
    "\n",
    "minimax_model = f\"{openrouter_prefix}{os.getenv(\"MINIMAX_FREE\")}\"\n",
    "zai_model = f\"{openrouter_prefix}{os.getenv(\"Z_AI_FREE\")}\"\n",
    "\n",
    "print(f\"Models: \\n1. {minimax_model} \\n2. {zai_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e8cd712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In *Hamlet*, when Laertes asks \"Where is my father?\" in **Act 4, Scene 5**, **Claudius** (the King) replies:\n",
       "\n",
       "> **\"Dead.\"**\n",
       "\n",
       "He then elaborates, saying, \"But there's no great cry. **He lies at ease.**\" (Some editions use \"He lies in peace\" instead of \"ease\").\n",
       "\n",
       "### Context:\n",
       "- Laertes has just returned from France after hearing his father, **Polonius**, was killed.\n",
       "- Ophelia, Laertes' sister, enters the scene, having been driven mad by grief. She sings and distributes flowers.\n",
       "- After Ophelia leaves, Laertes asks Claudius where Polonius is buried.\n",
       "- Claudius, who secretly ordered Polonius's death and hid the body, lies about the funeral and Polonius's fate to placate Laertes.\n",
       "- This exchange is part of Claudius' ongoing deception to prevent Laertes from seeking immediate revenge against Hamlet.\n",
       "\n",
       "### Why this matters:\n",
       "Claudius' lie (\"He lies at ease\") contrasts with the audience's knowledge that Polonius' body was hidden *unceremoniously* in a crypt, and it deepens the theme of concealment in the play. Laertes eventually learns the truth from the gravedigger in **Act 5, Scene 1**, fueling his role as Hamlet's foil in the fencing match."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = litellm.completion(model=minimax_model, messages=question)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e6f8a7",
   "metadata": {},
   "source": [
    "# Find the tokens and cost for previous call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cefca949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 59\n",
      "Output tokens: 602\n",
      "Total tokens: 661\n",
      "Reasoning tokens: 318\n",
      "Usages Details: Usage(completion_tokens=602, prompt_tokens=59, total_tokens=661, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=318, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)\n",
      "{'date': 'Fri, 07 Nov 2025 00:57:17 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'content-encoding': 'gzip', 'access-control-allow-origin': '*', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self \"https://checkout.stripe.com\" \"https://connect-js.stripe.com\" \"https://js.stripe.com\" \"https://*.js.stripe.com\" \"https://hooks.stripe.com\")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '99a8e01a5b8e7fc2-MAA'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Reasoning tokens: {response.usage.completion_tokens_details.reasoning_tokens}\")\n",
    "print(f\"Usages Details: {response.usage}\")\n",
    "\n",
    "print(response._response_headers)\n",
    "\n",
    "# Check how to retrieve below details from response\n",
    "# print(response.litellm_call_id)\n",
    "# print(response.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26500664",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "* It seems because we are using free models from openrouter caching is not supported there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "878c8ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Response: \n",
       "In *Hamlet* (Act 4, Scene 5), when Laertes bursts in demanding, \"**Where is my father?**\" (after Pol"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 59\n",
      "Output tokens: 839\n",
      "Total tokens: 898\n",
      "Reasoning tokens: 409\n"
     ]
    }
   ],
   "source": [
    "response = litellm.completion(model=minimax_model, messages=question)\n",
    "\n",
    "display(Markdown(f\"## Response: \\n{response.choices[0].message.content[:100]}\"))\n",
    "\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Reasoning tokens: {response.usage.completion_tokens_details.reasoning_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553f1f6",
   "metadata": {},
   "source": [
    "## Prompt Caching with OpenAI\n",
    "\n",
    "For OpenAI:\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-caching\n",
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.\n",
    "\n",
    "\n",
    "Cached input is 4X cheaper\n",
    "\n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25674908",
   "metadata": {},
   "source": [
    "## Prompt Caching with Anthropic\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\n",
    "\n",
    "You have to tell Claude what you are caching\n",
    "\n",
    "You pay 25% MORE to \"prime\" the cache\n",
    "\n",
    "Then you pay 10X less to reuse from the cache with inputs.\n",
    "\n",
    "https://www.anthropic.com/pricing#api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f1f8b",
   "metadata": {},
   "source": [
    "## Gemini supports both 'implicit' and 'explicit' prompt caching\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
