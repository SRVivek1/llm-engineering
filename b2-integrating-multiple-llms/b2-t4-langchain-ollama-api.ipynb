{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5643c90",
   "metadata": {},
   "source": [
    "# LangChain -  a First look at the powerful, mighty (and quite heaviweight) LangChain\n",
    "\n",
    "## API Docs\n",
    "* hhttps://api.smith.langchain.com/redoc\n",
    "* https://reference.langchain.com/python/integrations/langchain_ollama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30933d56",
   "metadata": {},
   "source": [
    "## Connect to LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4895f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779b686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully. \n",
      "OpenAI Key: sk-proj-qrUmGND... \n",
      "Model: gemma3:1b\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"GEMMA3_1B\")\n",
    "\n",
    "if not model_name:\n",
    "    print(\"Unable to load API configuration.\")\n",
    "else:\n",
    "    print(f\"Config loaded successfully. \\nOpenAI Key: {openai_api_key[:15]}... \\nModel: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176b5fb",
   "metadata": {},
   "source": [
    "## Connect to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce4ab613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement: Ollama serve running and pull required LLM models in local\n",
    "\n",
    "# num_gpu=0,  # Forces CPU-only (0 layers offloaded to GPU)\n",
    "client_def_gpu = ChatOllama(model=model_name, validate_model_on_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e06b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload\n",
    "tell_me_a_joke = [\n",
    "    {'role': 'system', 'content': 'you are a comedian and humourous person.'},\n",
    "    {'role': 'human', 'content': 'Tell a joke for a student on the journey to becoming an expert in LLM Engineering'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bdadd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here we go! Let’s dial up the chuckle factor.\n",
      "\n",
      "**(Adjusts microphone, leans into the camera with a slightly mischievous grin)**\n",
      "\n",
      "Alright, alright, settle down, future LLM gurus! You're tackling this thing, right? Becoming an expert in LLMs… it’s basically trying to teach a toddler to drive a spaceship. \n",
      "\n",
      "**(Pause for a beat, dramatic pause)**\n",
      "\n",
      "So, you’re learning about prompt engineering, right?  It’s like... explaining to a toaster *how* to make toast.  You’re feeding it incredibly specific, slightly confusing instructions.  \n",
      "\n",
      "**(Gestures wildly)**\n",
      "\n",
      "And it *still* produces… well, let’s just say it’s *interesting*. \n",
      "\n",
      "**(Leans in conspiratorially)**\n",
      "\n",
      "You’ll be spending hours tweaking the parameters, tweaking the temperature… all while the model just keeps repeating, “I don’t understand.  I don't understand.” \n",
      "\n",
      "**(Beat)**\n",
      "\n",
      "Seriously, it’s like the LLM is having a tiny existential crisis.  \n",
      "\n",
      "**(Quick, slightly exasperated chuckle)**\n",
      "\n",
      "So, congratulations on mastering the art of the 'hold your horses' command!  You're getting there! \n",
      "\n",
      "---\n",
      "\n",
      "**Want to hear another one? Maybe something a little… darker?** (I can whip up a joke about biased data if you like!)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "respose = client_def_gpu.invoke(input=tell_me_a_joke)\n",
    "\n",
    "print(respose.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbe1fe",
   "metadata": {},
   "source": [
    "## Below Model throws error for low GPU Memory\n",
    "* Hence explicitly setting to CPU only mode\n",
    "\n",
    "### Error:\n",
    "* **ResponseError:** *model requires more system memory than is currently available unable to load full model on GPU (status code: 500)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the LLM (Large Language Model) go to therapy?\n",
      "\n",
      "Because it was struggling with its \"contextualization\" issues! It kept having trouble understanding the bigger picture and would often get caught up in its own \"loop\" of generated text. But don't worry, the therapist just told it to \"retrain\" its perspective and work on its ability to \"summarize\" its thoughts!\n",
      "\n",
      "(Sorry, I know, I know, it's a bit of a \"model\" mistake... but hey, someone's gotta keep you students in stitches while you're learning about transformer architectures and masked language modeling!)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deepseek_r1_8b_model = os.getenv(\"LLAMA3_3B\")\n",
    "\n",
    "# num_gpu=0,  # Forces CPU-only (0 layers offloaded to GPU)\n",
    "client = ChatOllama(model=deepseek_r1_8b_model, validate_model_on_init=True, num_gpu=0, temperature=0.7)\n",
    "\n",
    "resp = client.invoke(tell_me_a_joke)\n",
    "\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de14191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, gather 'round, aspiring LLM architects! So, you're training this fancy new model, right? It's got layers upon layers, billions of parameters, you've poured over the datasets, fine-tuned until your eyes are bleedin'. And you're just... waiting... for it to actually *do* something useful.\n",
      "\n",
      "You've got your coffee pot on, maybe even a nice, aromatic brew brewing...\n",
      "\n",
      "And you're just sittin' there, nervously checking the time, checkin' the GPU usage, checkin' the progress bar... it's *slow*.\n",
      "\n",
      "And then... it happens. The coffee pot clicks. The whistling begins. The warmth starts to fill the room.\n",
      "\n",
      "And at that exact same moment... BOOM! Your model finally generates a coherent, insightful, and actually *correct* response!\n",
      "\n",
      "The punchline? It happened because the coffee was ready.\n",
      "\n",
      "*(Leans into the mic)* You see, you can't rush training a model. You gotta let it simmer, just like the perfect brew. You wait for the data, you wait for the computation, you wait... and then, like magic, it *clicks*. Or, you know, it was the caffeine. It always is.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deepseek_r1_8b_model = os.getenv(\"DEEPSEEK_R1_8B\")\n",
    "\n",
    "# num_gpu=0,  # Forces CPU-only (0 layers offloaded to GPU)\n",
    "client = ChatOllama(model=deepseek_r1_8b_model, validate_model_on_init=True, num_gpu=0, temperature=0.7)\n",
    "\n",
    "resp = client.invoke(tell_me_a_joke)\n",
    "\n",
    "print(resp.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
