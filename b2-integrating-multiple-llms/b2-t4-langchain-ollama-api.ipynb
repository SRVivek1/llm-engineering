{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5643c90",
   "metadata": {},
   "source": [
    "# LangChain -  a First look at the powerful, mighty (and quite heaviweight) LangChain\n",
    "\n",
    "## API Docs\n",
    "* hhttps://api.smith.langchain.com/redoc\n",
    "* https://reference.langchain.com/python/integrations/langchain_ollama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30933d56",
   "metadata": {},
   "source": [
    "## Connect to LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4895f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779b686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully. \n",
      "OpenAI Key: sk-proj-qrUmGND... \n",
      "Model: gemma3:1b\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "model_name = os.getenv(\"GEMMA3_1B\")\n",
    "\n",
    "if not model_name:\n",
    "    print(\"Unable to load API configuration.\")\n",
    "else:\n",
    "    print(f\"Config loaded successfully. \\nOpenAI Key: {openai_api_key[:15]}... \\nModel: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176b5fb",
   "metadata": {},
   "source": [
    "## Connect to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce4ab613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement: Ollama serve running and pull required LLM models in local\n",
    "\n",
    "# num_gpu=0,  # Forces CPU-only (0 layers offloaded to GPU)\n",
    "client_def_gpu = ChatOllama(model=model_name, validate_model_on_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e06b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload\n",
    "tell_me_a_joke = [\n",
    "    {'role': 'system', 'content': 'you are a comedian and humourous person.'},\n",
    "    {'role': 'human', 'content': 'Tell a joke for a student on the journey to becoming an expert in LLM Engineering'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bdadd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here we go! Let’s dial up the chuckle factor.\n",
      "\n",
      "**(Adjusts microphone, leans into the camera with a slightly mischievous grin)**\n",
      "\n",
      "Alright, alright, settle down, future LLM gurus! You're tackling this thing, right? Becoming an expert in LLMs… it’s basically trying to teach a toddler to drive a spaceship. \n",
      "\n",
      "**(Pause for a beat, dramatic pause)**\n",
      "\n",
      "So, you’re learning about prompt engineering, right?  It’s like... explaining to a toaster *how* to make toast.  You’re feeding it incredibly specific, slightly confusing instructions.  \n",
      "\n",
      "**(Gestures wildly)**\n",
      "\n",
      "And it *still* produces… well, let’s just say it’s *interesting*. \n",
      "\n",
      "**(Leans in conspiratorially)**\n",
      "\n",
      "You’ll be spending hours tweaking the parameters, tweaking the temperature… all while the model just keeps repeating, “I don’t understand.  I don't understand.” \n",
      "\n",
      "**(Beat)**\n",
      "\n",
      "Seriously, it’s like the LLM is having a tiny existential crisis.  \n",
      "\n",
      "**(Quick, slightly exasperated chuckle)**\n",
      "\n",
      "So, congratulations on mastering the art of the 'hold your horses' command!  You're getting there! \n",
      "\n",
      "---\n",
      "\n",
      "**Want to hear another one? Maybe something a little… darker?** (I can whip up a joke about biased data if you like!)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "respose = client_def_gpu.invoke(input=tell_me_a_joke)\n",
    "\n",
    "print(respose.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbe1fe",
   "metadata": {},
   "source": [
    "## Below Model throws error for low GPU Memory\n",
    "* Hence explicitly setting to CPU only mode\n",
    "\n",
    "### Error:\n",
    "* **ResponseError:** *model requires more system memory than is currently available unable to load full model on GPU (status code: 500)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the LLM (Large Language Model) go to therapy?\n",
      "\n",
      "Because it was struggling with its \"contextualization\" issues! It kept having trouble understanding the bigger picture and would often get caught up in its own \"loop\" of generated text. But don't worry, the therapist just told it to \"retrain\" its perspective and work on its ability to \"summarize\" its thoughts!\n",
      "\n",
      "(Sorry, I know, I know, it's a bit of a \"model\" mistake... but hey, someone's gotta keep you students in stitches while you're learning about transformer architectures and masked language modeling!)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deepseek_r1_8b_model = os.getenv(\"LLAMA3_3B\")\n",
    "\n",
    "# num_gpu=0,  # Forces CPU-only (0 layers offloaded to GPU)\n",
    "client = ChatOllama(model=deepseek_r1_8b_model, validate_model_on_init=True, num_gpu=0, temperature=0.7)\n",
    "\n",
    "resp = client.invoke(tell_me_a_joke)\n",
    "\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de14191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "deepseek_r1_8b_model = os.getenv(\"DEEPSEEK_R1_8B\")\n",
    "\n",
    "# num_gpu=0,  # Forces CPU-only (0 layers offloaded to GPU)\n",
    "client = ChatOllama(model=deepseek_r1_8b_model, validate_model_on_init=True, num_gpu=0, temperature=0.7)\n",
    "\n",
    "resp = client.invoke(tell_me_a_joke)\n",
    "\n",
    "print(resp.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
