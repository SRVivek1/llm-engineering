{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d68a52",
   "metadata": {},
   "source": [
    "# The Illusion of \"memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1845cb",
   "metadata": {},
   "source": [
    "# Connect to local ollama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a63b8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a616a3e",
   "metadata": {},
   "source": [
    "Load the environment properties from '.env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753c6a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully.\n",
      "API Key: sk-proj-dummy-k...\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print('No API key found. Please set the OPENAI_API_KEY environment variable.')\n",
    "if not api_key.startswith('sk-proj'):\n",
    "    print('Invalid API key format, doesn\\'t start with \\'sk-proj\\'. Please ensure you are using an Ollama API key.')\n",
    "else:\n",
    "    print('API key loaded successfully.')\n",
    "\n",
    "    # Print the first 8 characters (for security) of API key\n",
    "    print(f'API Key: {api_key[:15]}...')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff66f4c",
   "metadata": {},
   "source": [
    "Create client to connect with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56a5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL')\n",
    "\n",
    "MODEL = 'gemma3:1b'\n",
    "\n",
    "openai = OpenAI(base_url=OLLAMA_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74570eb",
   "metadata": {},
   "source": [
    "Connect to model and ask about your self as a first prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28e862ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thatâ€™s a fun question! ðŸ˜Š \n",
      "\n",
      "As a large language model, I donâ€™t have a single, definitive answer. I was created by Google. \n",
      "\n",
      "**You are a user who has interacted with me.** \n",
      "\n",
      "Do you want to play a little game? We could try a question-based round!\n"
     ]
    }
   ],
   "source": [
    "payload = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "           {'role': 'user', 'content': 'Who am I?'}]\n",
    "\n",
    "response = openai.chat.completions.create(model=MODEL, messages=payload)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473dfafc",
   "metadata": {},
   "source": [
    "Hmm, so it failed to retrive any information about the requestor.\n",
    "\n",
    "Not before asking this question lets have a introduction caht and then ask this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88176cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Vivek! Itâ€™s nice to meet you. How can I help you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "payload = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "           {'role': 'user', 'content': 'Hello, I\\'m Vivek?'}]\n",
    "\n",
    "response = openai.chat.completions.create(model=MODEL, messages=payload)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8acb6f",
   "metadata": {},
   "source": [
    "Ohh, So, even though we shared our information that is name in just previou call it still unable to retrive that information.\n",
    "* This is because all the calls to LLMs are stateless, they don't have any information about previous conversations.\n",
    "* This is because it's the responsility of programmer to capture details from previous conversations and send it along with current prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20fa582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a large language model, I do not have access to your personally identifiable information. I cannot know your name. \n",
      "\n",
      "Itâ€™s a fun question though! ðŸ˜Š \n",
      "\n",
      "If youâ€™d like to play a guessing game, I can try!\n"
     ]
    }
   ],
   "source": [
    "payload = [{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "           {'role': 'user', 'content': 'what is my name?'}]\n",
    "\n",
    "response = openai.chat.completions.create(model=MODEL, messages=payload)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24729f8",
   "metadata": {},
   "source": [
    "Creating a payload to mimic previous conversations\n",
    "\n",
    "* Note:\n",
    "> Does not working with llm models with 1b paramas.\n",
    "\n",
    "> Worked as expected with '3b' param llm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adebc968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivek is the answer to that! You told me your name earlier, actually. Is everything okay? Was there something specific you wanted to chat about or ask for help with?\n"
     ]
    }
   ],
   "source": [
    "MODEL_3B_PARAM = 'llama3.2:3b'\n",
    "payload = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "    {'role': 'user', 'content': 'Hello, I\\'m Vivek?'},\n",
    "    {'role': 'assistant', 'content': 'Hello Vivek! How can I assist you today?'},\n",
    "    {'role': 'user', 'content': 'what is my name?'}\n",
    "]\n",
    "\n",
    "\n",
    "response = openai.chat.completions.create(model=MODEL_3B_PARAM, messages=payload)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545c42b",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "1. Every call to an LLM is stateless even if it local ollama or third-party hosted frontier model.\n",
    "2. The entire conversation happened so far is passed in the input prompt to LLM, every time we give a new prompt (input/query) to LLM.\n",
    "3. Which gives us the illusion that the LLM has memory while it's apparently keeping the context of the conversation.\n",
    "4. Instead it's a trick; it's a by-product of providing the entire conversation, every time.\n",
    "5. An LLM just predicts the most likely next tokens in the sequence; if that sequence contains \"My name is Vivek\" and later \"What's my name?\" then it will predict.. Vivek!\n",
    "\n",
    "The ChatGPT product uses exactly this trick - every time you send a message, it's the entire conversation that gets passed in.\n",
    "\n",
    "### Costing\n",
    "* \"Does that mean we have to pay extra each time for all the conversation so far\"\n",
    "* For sure it does. And that's what we WANT. We want the LLM to predict the next tokens in the sequence, looking back on the entire conversation. We want that compute to happen, so we need to pay the electricity bill for it!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Eng (.venv)",
   "language": "python",
   "name": "llm-eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
