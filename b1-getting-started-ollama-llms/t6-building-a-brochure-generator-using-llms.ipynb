{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ea9033",
   "metadata": {},
   "source": [
    "# Brochure Generator - A fully funcation business solution\n",
    "\n",
    "### Business Requirement:\n",
    "\n",
    "* Develope an application that builds a Brochure for a company to be used for prospective clients, investors and potential recruits.\n",
    "* We will be provided a company name and their primary website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352e60a",
   "metadata": {},
   "source": [
    "# Initial setup of imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25176de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import json\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Local module imports\n",
    "import scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a3488",
   "metadata": {},
   "source": [
    "# Load env properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# check api key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if(api_key and gemini_key and len(api_key) > 10 and len(gemini_key) > 10):\n",
    "    print(\"API key look good.\")\n",
    "    print(f\"Gemini Key: {gemini_key}, {'\\n'}OpenAI Key: {api_key}.\")\n",
    "else:\n",
    "    print(\"No API key found. Please set OPENAI_API_KEY or GEMINI_API_KEY in your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579f3efe",
   "metadata": {},
   "source": [
    "Construct client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\")\n",
    "\n",
    "# Construct client\n",
    "ollama_client = OpenAI(api_key=api_key, base_url=OLLAMA_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495b6a6",
   "metadata": {},
   "source": [
    "Test the scraper to load links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = scraper.fetch_website_links(\"https://edwarddonner.com\")\n",
    "\n",
    "# Print all extracted links\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f6f09",
   "metadata": {},
   "source": [
    "## First step: Use the LLM Models to figure out relevant links\n",
    "* The LLM should analyse all the extractd links from website and replace relative links such as \"/products\" with \"https://companydomain.com/products\"\n",
    "* In this app we'll use `One shot prompting` where we'll provide and example that how it should respond in the prompt.\n",
    "* It's an excellent example of LLM use case, because it requires naunced understanding. Imagine the level of work required if we have to write application to parse and analyse all the links manually. It will be very difficult.\n",
    "\n",
    ">**Note:** *There ia a more advance technique called `Structured Outputs` in which we require the model to response acording to a spec. Which we'll be discussing in upcoming sessions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39efb7",
   "metadata": {},
   "source": [
    "# Write System and user prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7f5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK_ANALYSIS_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that helps to analyse website links and convert relative links to absolute links based on the main domain.\n",
    "You are able to decide which of the links would be most relevant to include in a brochure about the company, such as links to an About \n",
    "page, or a Company page, or Career/Jobs page.\n",
    "\n",
    "You should respond in JSON format with the following structure:\n",
    "{\n",
    "  \"relevant_links\": [\n",
    "    {\"name\": \"about page\",, url\": \"https://companydomain.com/about\", \"reason\": \"This page tells about the company mission and values.\"},\n",
    "    {\"name\": \"careers page\", \"url\": \"https://companydomain.com/careers\", \"reason\": \"This page provides information about job opportunities.\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85457331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build user prompt by including all the links extracted from the website\n",
    "def get_links_user_prompt(url):\n",
    "    user_prompt = f\"\"\"\n",
    "    Here is a list of links extracted from the website {url}:\n",
    "    Please analyse and decide which of these links are relevant web links for a brochure about the company.\n",
    "    Replace any relative links with absolute links based on the main domain {url}.\n",
    "    Do not include Terms of Service, Privacy Policy, Cookie Policy, or email links.\n",
    "\n",
    "    # Links (some might be relative links):\n",
    "    \"\"\"\n",
    "\n",
    "    links = scraper.fetch_website_links(url)\n",
    "    user_prompt += \"\\n\".join(links)\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d45fd",
   "metadata": {},
   "source": [
    "Test above user_prompt API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_links_user_prompt(\"https://edwarddonner.com\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb718922",
   "metadata": {},
   "source": [
    "Function to interate LLM and analyse for relative links for brochure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate with LLM model to select relevant links\n",
    "def select_relevant_links_ollama(ollama_model, url):\n",
    "    print(f\"Using model: {ollama_model}\")\n",
    "    payload = [\n",
    "        {\"role\": \"system\", \"content\": LINK_ANALYSIS_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": get_links_user_prompt(url)}\n",
    "    ]\n",
    "\n",
    "    # Instruct model to respond in JSON format\n",
    "    json_resp_format = {\"type\": \"json_object\"}\n",
    "\n",
    "    response = ollama_client.chat.completions.create(model=ollama_model, messages=payload, response_format=json_resp_format)\n",
    "    result = response.choices[0].message.content\n",
    "\n",
    "    links = json.loads(result)\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146bd2b6",
   "metadata": {},
   "source": [
    "Call above API to get the relevant links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA3_3B_MODEL_KEY = \"LLAMA3_3B\"\n",
    "ollama_model = os.getenv(LLAMA3_3B_MODEL_KEY)\n",
    "\n",
    "if(not ollama_model):\n",
    "    print(f\"No model defined with name {LLAMA3_3B_MODEL_KEY} in .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24e5f85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relevant_links': [{'name': 'Home page',\n",
       "   'url': 'https://edwarddonner.com/',\n",
       "   'reason': 'This is the main homepage of the company.'},\n",
       "  {'name': 'About me and About Nebula',\n",
       "   'url': 'https://edwarddonner.com/about-me-and-about-nebula/',\n",
       "   'reason': 'This page tells about the author and his projects.'},\n",
       "  {'name': 'Connect our four games',\n",
       "   'url': 'https://edwarddonner.com/connect-four/',\n",
       "   'reason': \"This page provides information about one of the company's four games.\"},\n",
       "  {'name': 'Expert advice',\n",
       "   'url': 'https://edwarddonner.com/2025/09/15/ai-in-production-gen-ai-and-agentic-ai-on-aws-at-scale/',\n",
       "   'reason': 'This page shares insights on AI in production.'},\n",
       "  {'name': 'Learning resources',\n",
       "   'url': 'https://edwarddonner.com/2025/05/28/connecting-my-courses-become-an-llm-expert-and-leader/',\n",
       "   'reason': 'This page provides information about online courses.'},\n",
       "  {'name': 'Industry news and updates',\n",
       "   'url': 'https://www.prnewswire.com/news-releases/wynden-stark-group-acquires-nyc-venture-backed-tech-startup-untapt-301269512.html'},\n",
       "  {'name': 'Social profiles',\n",
       "   'url': 'https://www.linkedin.com/in/eddonner/',\n",
       "   'reason': \"This link provides access to the author's LinkedIn profile.\"},\n",
       "  {'name': 'Twitter profile',\n",
       "   'url': 'https://twitter.com/edwarddonner',\n",
       "   'reason': \"This link leads to the company's author's Twitter profile.\"},\n",
       "  {'name': 'Facebook profile',\n",
       "   'url': 'https://www.facebook.com/edward.donner.52',\n",
       "   'reason': \"This link provides access to the company's author's Facebook profile.\"}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_relevant_links_ollama(ollama_model, \"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a29057",
   "metadata": {},
   "source": [
    "# Test above websites\n",
    "\n",
    "## Observation\n",
    "* It's halucinating, appoitment, feature and freemium links are embedded in one single link, creating and invalid URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71063053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'relevant_links': [{'name': 'Meet the Team',\n",
       "   'url': 'https://nebula.io/meet-the-team',\n",
       "   'reason': 'This page provides information about the company people.'},\n",
       "  {'name': 'Contact Us',\n",
       "   'url': 'https://nebula.io/contact',\n",
       "   'reason': 'This page provides information on how to get in touch with the company.'},\n",
       "  {'name': 'Support Resources',\n",
       "   'url': 'https://nebula.io/resources-1',\n",
       "   'reason': 'These resources provide assistance and support for customers.'},\n",
       "  {'name': 'Frequently Asked Questions',\n",
       "   'url': 'https://nebula.io/frequently-asked-questions',\n",
       "   'reason': 'This page answers common questions about the company or its services.'}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_relevant_links_ollama(ollama_model, \"https://nebula.io/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaaecf2",
   "metadata": {},
   "source": [
    "Another function to connect with Any Frontier Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "803b58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate with Any LLM model to select relevant links\n",
    "def select_relevant_links(base_url, model_name, api_key, website_url):\n",
    "    \n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    print(f\"Using model: {model_name}\")\n",
    "\n",
    "    payload = [\n",
    "        {\"role\": \"system\", \"content\": LINK_ANALYSIS_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": get_links_user_prompt(website_url)}\n",
    "    ]\n",
    "\n",
    "    # Instruct model to respond in JSON format\n",
    "    json_resp_format = {\"type\": \"json_object\"}\n",
    "\n",
    "    response = client.chat.completions.create(model=model_name, messages=payload, response_format=json_resp_format)\n",
    "    result = response.choices[0].message.content\n",
    "\n",
    "    links = json.loads(result)\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dfb242",
   "metadata": {},
   "source": [
    "Connect with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b96e41a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gemini-2.5-flash\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'relevant_links': [{'name': 'homepage',\n",
       "   'url': 'https://nebula.io/',\n",
       "   'reason': 'Provides an initial overview of the company and its main services.'},\n",
       "  {'name': 'features page',\n",
       "   'url': 'https://nebula.io/features',\n",
       "   'reason': 'Highlights the core functionalities and services offered by the company.'},\n",
       "  {'name': 'resources page',\n",
       "   'url': 'https://nebula.io/resources',\n",
       "   'reason': \"Offers valuable information, insights, or support related to the company's products/services.\"},\n",
       "  {'name': 'frequently asked questions page',\n",
       "   'url': 'https://nebula.io/frequently-asked-questions',\n",
       "   'reason': 'Addresses common questions about the company, its offerings, and general operations.'},\n",
       "  {'name': 'contact page',\n",
       "   'url': 'https://nebula.io/contact',\n",
       "   'reason': 'Provides essential contact information for inquiries, partnerships, or support.'},\n",
       "  {'name': 'meet the team page',\n",
       "   'url': 'https://nebula.io/meet-the-team',\n",
       "   'reason': \"Introduces the company's leadership and team, fostering trust and transparency (similar to an 'About Us' section).\"},\n",
       "  {'name': 'freemium offering page',\n",
       "   'url': 'https://nebula.io/freemium',\n",
       "   'reason': 'Describes a key offering or business model of the company, showing how users can engage with their product.'}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = os.getenv(\"GEMINI_BASE_URL\")\n",
    "gemini_model = os.getenv(\"GEMINI_FM\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "website_url = \"https://nebula.io/\"\n",
    "\n",
    "select_relevant_links(base_url=base_url, model_name=gemini_model, api_key=gemini_api_key, website_url=website_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05825c",
   "metadata": {},
   "source": [
    "Analyzing with ollama LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20078243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: llama3.2:3b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'relevant_links': [{'name': 'About Us',\n",
       "   'url': 'https://huggingface.co/about',\n",
       "   'reason': 'This page tells about the company mission and values.'},\n",
       "  {'name': 'Company',\n",
       "   'url': 'https://huggingface.co/company',\n",
       "   'reason': 'This page provides an overview of the company.'},\n",
       "  {'name': 'Careers',\n",
       "   'url': 'https://huggingface.co/careers',\n",
       "   'reason': 'This page provides information about job opportunities.'},\n",
       "  {'name': 'Blog',\n",
       "   'url': 'https://discuss.huggingface.co',\n",
       "   'reason': 'This page showcases the blog posts of the company.'},\n",
       "  {'name': 'GitHub',\n",
       "   'url': 'https://github.com/huggingface',\n",
       "   'reason': 'This page allows developers to explore and contribute to the Hugging Face projects'},\n",
       "  {'name': '',\n",
       "   'url': 'https://twitter.com/huggingface',\n",
       "   'reason': '-related social media handle, not a core company webpage.'}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_relevant_links_ollama(url=\"https://huggingface.co\", ollama_model=ollama_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25513fa8",
   "metadata": {},
   "source": [
    "Google Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8549f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gemini-2.5-flash\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'relevant_links': [{'name': 'Home page',\n",
       "   'url': 'https://huggingface.co/',\n",
       "   'reason': 'Serves as the main landing page and general overview of the company.'},\n",
       "  {'name': 'Enterprise solutions page',\n",
       "   'url': 'https://huggingface.co/enterprise',\n",
       "   'reason': 'Provides information on solutions tailored for businesses and organizations.'},\n",
       "  {'name': 'Pricing page',\n",
       "   'url': 'https://huggingface.co/pricing',\n",
       "   'reason': 'Details the costs and plans for various services and products offered by the company.'},\n",
       "  {'name': 'Careers page',\n",
       "   'url': 'https://apply.workable.com/huggingface/',\n",
       "   'reason': 'Lists available job opportunities and information about working at the company.'},\n",
       "  {'name': 'Brand page',\n",
       "   'url': 'https://huggingface.co/brand',\n",
       "   'reason': \"Outlines the company's brand identity and values, which is important for understanding the company's image.\"},\n",
       "  {'name': 'Learn page',\n",
       "   'url': 'https://huggingface.co/learn',\n",
       "   'reason': \"Offers educational resources, showcasing the company's commitment to knowledge sharing and user empowerment.\"},\n",
       "  {'name': 'Company blog',\n",
       "   'url': 'https://huggingface.co/blog',\n",
       "   'reason': 'Features company news, updates, technical insights, and announcements.'},\n",
       "  {'name': 'LinkedIn company profile',\n",
       "   'url': 'https://www.linkedin.com/company/huggingface/',\n",
       "   'reason': 'Provides a professional overview of the company, its culture, and updates.'},\n",
       "  {'name': 'Documentation page',\n",
       "   'url': 'https://huggingface.co/docs',\n",
       "   'reason': \"Offers comprehensive guides and references for the company's products and tools.\"},\n",
       "  {'name': 'Models page',\n",
       "   'url': 'https://huggingface.co/models',\n",
       "   'reason': \"Showcases the company's core offering in pre-trained models.\"},\n",
       "  {'name': 'Datasets page',\n",
       "   'url': 'https://huggingface.co/datasets',\n",
       "   'reason': \"Highlights the company's core offering in datasets for machine learning.\"},\n",
       "  {'name': 'Spaces page',\n",
       "   'url': 'https://huggingface.co/spaces',\n",
       "   'reason': \"Demonstrates the company's platform for building and sharing ML applications.\"}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base_url = os.getenv(\"GEMINI_BASE_URL\")\n",
    "gemini_model = os.getenv(\"GEMINI_FM\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "website_url = \"https://huggingface.co\"\n",
    "\n",
    "select_relevant_links(base_url=base_url, model_name=gemini_model, api_key=gemini_api_key, website_url=website_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116633d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Second Step: Make the broucher!\n",
    "Assembel all the details info another prompt to LLMs\n",
    "* API\n",
    "  * Extract text content of website using scraper and initialize result\n",
    "  * Get all links and filter relevant one's using Ollama LLM model\n",
    "  * Read info about each link using scraper and append in result\n",
    "  * Use Markdown formatting for make it presentable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "319a6e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create brochure using selected links\n",
    "def fetch_page_and_all_relevant_links(ollama_model, website_url):\n",
    "    content = scraper.fetch_text_contents(website_url)\n",
    "    ollama_model = os.getenv(LLAMA3_3B_MODEL_KEY)\n",
    "    relevant_links = select_relevant_links_ollama(ollama_model, website_url)\n",
    "    print(relevant_links)\n",
    "    \n",
    "    result = f\"## Landing Page: \\n\\n{content}\\n---\\n## Relevnant Links:\\n\\n\"\n",
    "    for link in relevant_links['relevant_links']:\n",
    "        result += f\"\\n\\n### Link: {link['name']}\\n\"\n",
    "        result += scraper.fetch_text_contents(link['url'])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763cac61",
   "metadata": {},
   "source": [
    "Call above API\n",
    "\n",
    "> Hallucinating\n",
    "*  Created a invald URL, caused failure - https://discuss.huggingface.co/topics/career-development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a16cd55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: llama3.2:3b\n",
      "{'relevant_links': [{'name': 'About Page', 'url': 'https://huggingface.co/about', 'reason': 'This page provides information about the company mission and values.'}, {'name': 'Company Page', 'url': 'https://huggingface.co/blog', 'reason': 'This page provides various blog posts and updates from Hugging Face'}, {'name': 'Career/Jobs Page', 'url': 'https://discuss.huggingface.co/topics/career-development', 'reason': 'This page provides information about job opportunities at Hugging Face.'}]}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://discuss.huggingface.co/topics/career-development",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfetch_page_and_all_relevant_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mollama_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://huggingface.co\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mfetch_page_and_all_relevant_links\u001b[39m\u001b[34m(ollama_model, website_url)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m relevant_links[\u001b[33m'\u001b[39m\u001b[33mrelevant_links\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     10\u001b[39m     result += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m### Link: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     result += \u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_text_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/llm-engineering/b1-getting-started-ollama-llms/scraper.py:58\u001b[39m, in \u001b[36mfetch_text_contents\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03mReturn the title and contents of the website at the given url,\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03mtruncate to 2,000 characters as a sensible limit\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SOUP \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m URL != url:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[43m__fetch_website_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Read all text content from the webpage\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SOUP \u001b[38;5;129;01mand\u001b[39;00m SOUP.body:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# Remove unwanted contents\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/llm-engineering/b1-getting-started-ollama-llms/scraper.py:38\u001b[39m, in \u001b[36m__fetch_website_contents\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     35\u001b[39m response = requests.get(url, headers=HEADERS)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Raise an error for bad responses\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Parse the HTML content using BeautifulSoup\u001b[39;00m\n\u001b[32m     41\u001b[39m SOUP = BeautifulSoup(response.content, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://discuss.huggingface.co/topics/career-development"
     ]
    }
   ],
   "source": [
    "print(fetch_page_and_all_relevant_links(ollama_model, \"https://huggingface.co\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9abe2",
   "metadata": {},
   "source": [
    "Create another function to connect with any Frontier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c0dd572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create brochure using selected links\n",
    "def fetch_page_and_all_relevant_links_fmodel(api_key, base_url, model_name, website_url):\n",
    "    content = scraper.fetch_text_contents(website_url)\n",
    "    relevant_links = select_relevant_links(base_url=base_url, model_name=model_name, api_key=api_key, website_url=website_url)\n",
    "    #print(relevant_links)\n",
    "\n",
    "    result = f\"## Landing Page: \\n\\n{content}\\n---\\n## Relevnant Links:\\n\\n\"\n",
    "    for link in relevant_links['relevant_links']:\n",
    "        result += f\"\\n\\n### Link: {link['name']}\\n\"\n",
    "        result += scraper.fetch_text_contents(link['url'])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58db745",
   "metadata": {},
   "source": [
    "Call Gemini LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9a1f891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gemini-2.5-flash\n",
      "## Landing Page: \n",
      "\n",
      "Hugging Face ‚Äì The AI community building the future.\n",
      "\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "The AI community building the future.\n",
      "The platform where the machine learning community collaborates on models, datasets, and applications.\n",
      "Explore AI Apps\n",
      "or\n",
      "Browse 1M+ models\n",
      "Trending on\n",
      "this week\n",
      "Models\n",
      "MiniMaxAI/MiniMax-M2\n",
      "Updated\n",
      "5 days ago\n",
      "‚Ä¢\n",
      "726k\n",
      "‚Ä¢\n",
      "977\n",
      "deepseek-ai/DeepSeek-OCR\n",
      "Updated\n",
      "10 days ago\n",
      "‚Ä¢\n",
      "2.06M\n",
      "‚Ä¢\n",
      "2.41k\n",
      "moonshotai/Kimi-Linear-48B-A3B-Instruct\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "15k\n",
      "‚Ä¢\n",
      "319\n",
      "briaai/FIBO\n",
      "Updated\n",
      "about 24 hours ago\n",
      "‚Ä¢\n",
      "2.84k\n",
      "‚Ä¢\n",
      "193\n",
      "dx8152/Qwen-Edit-2509-Multiple-angles\n",
      "Updated\n",
      "about 8 hours ago\n",
      "‚Ä¢\n",
      "157\n",
      "Browse 1M+ models\n",
      "Spaces\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "1.15k\n",
      "1.15k\n",
      "The Smol Training Playbook: The Secrets to Building World-Class LLMs\n",
      "üìù\n",
      "Running\n",
      "15.6k\n",
      "15.6k\n",
      "DeepSite v3\n",
      "üê≥\n",
      "Generate any application by Vibe Coding\n",
      "Running\n",
      "2.25k\n",
      "2.25k\n",
      "Wan2.2 Animate\n",
      "üëÅ\n",
      "Wan2.2 Animate\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "MCP\n",
      "114\n",
      "114\n",
      "Dream-wan2-2-faster-Pro\n",
      "üé¨\n",
      "Generate a video from an image with detailed prompts\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "MCP\n",
      "2.03k\n",
      "2.03k\n",
      "Wan2.2 14B Fast\n",
      "üé•\n",
      "generate a video from an image with a text prompt\n",
      "Browse 400k+ applications\n",
      "Datasets\n",
      "nvidia/PhysicalAI-Autonomous-Vehicles\n",
      "Updated\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "8.9k\n",
      "‚Ä¢\n",
      "157\n",
      "HuggingFaceFW/finewiki\n",
      "Updated\n",
      "12 days ago\n",
      "‚Ä¢\n",
      "12.8k\n",
      "‚Ä¢\n",
      "200\n",
      "neulab/agent-data-collection\n",
      "Updated\n",
      "Sep 9\n",
      "‚Ä¢\n",
      "4.06k\n",
      "‚Ä¢\n",
      "52\n",
      "nvidia/Nemotron-VLM-Dataset-v2\n",
      "Updated\n",
      "4 days ago\n",
      "‚Ä¢\n",
      "1.7k\n",
      "‚Ä¢\n",
      "41\n",
      "fka/awesome-chatgpt-prompts\n",
      "Updated\n",
      "Jan 6\n",
      "‚Ä¢\n",
      "35.5k\n",
      "‚Ä¢\n",
      "9.33k\n",
      "Browse 250k+ datasets\n",
      "The Home of Machine Learning\n",
      "Create, discover and collaborate on ML better.\n",
      "The collaboration platform\n",
      "Host and collaborate on unlimited public models, datasets and applications.\n",
      "Move faster\n",
      "With the HF Open source stack.\n",
      "Explore all modalities\n",
      "Text, image, video, audio or even 3D.\n",
      "Build your portfolio\n",
      "Share your work with the world and build your ML profile.\n",
      "Sign Up\n",
      "Accelerate your ML\n",
      "We provide paid Compute and Enterprise solutions.\n",
      "Team & Enterprise\n",
      "Give your team the most advanced platform to build AI with enterpr\n",
      "---\n",
      "## Relevnant Links:\n",
      "\n",
      "\n",
      "\n",
      "### Link: Company Homepage\n",
      "Hugging Face ‚Äì The AI community building the future.\n",
      "\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "The AI community building the future.\n",
      "The platform where the machine learning community collaborates on models, datasets, and applications.\n",
      "Explore AI Apps\n",
      "or\n",
      "Browse 1M+ models\n",
      "Trending on\n",
      "this week\n",
      "Models\n",
      "MiniMaxAI/MiniMax-M2\n",
      "Updated\n",
      "5 days ago\n",
      "‚Ä¢\n",
      "726k\n",
      "‚Ä¢\n",
      "977\n",
      "deepseek-ai/DeepSeek-OCR\n",
      "Updated\n",
      "10 days ago\n",
      "‚Ä¢\n",
      "2.06M\n",
      "‚Ä¢\n",
      "2.41k\n",
      "moonshotai/Kimi-Linear-48B-A3B-Instruct\n",
      "Updated\n",
      "2 days ago\n",
      "‚Ä¢\n",
      "15k\n",
      "‚Ä¢\n",
      "319\n",
      "briaai/FIBO\n",
      "Updated\n",
      "about 24 hours ago\n",
      "‚Ä¢\n",
      "2.84k\n",
      "‚Ä¢\n",
      "193\n",
      "dx8152/Qwen-Edit-2509-Multiple-angles\n",
      "Updated\n",
      "about 8 hours ago\n",
      "‚Ä¢\n",
      "157\n",
      "Browse 1M+ models\n",
      "Spaces\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "1.15k\n",
      "1.15k\n",
      "The Smol Training Playbook: The Secrets to Building World-Class LLMs\n",
      "üìù\n",
      "Running\n",
      "15.6k\n",
      "15.6k\n",
      "DeepSite v3\n",
      "üê≥\n",
      "Generate any application by Vibe Coding\n",
      "Running\n",
      "2.25k\n",
      "2.25k\n",
      "Wan2.2 Animate\n",
      "üëÅ\n",
      "Wan2.2 Animate\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "MCP\n",
      "114\n",
      "114\n",
      "Dream-wan2-2-faster-Pro\n",
      "üé¨\n",
      "Generate a video from an image with detailed prompts\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "MCP\n",
      "2.03k\n",
      "2.03k\n",
      "Wan2.2 14B Fast\n",
      "üé•\n",
      "generate a video from an image with a text prompt\n",
      "Browse 400k+ applications\n",
      "Datasets\n",
      "nvidia/PhysicalAI-Autonomous-Vehicles\n",
      "Updated\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "8.9k\n",
      "‚Ä¢\n",
      "157\n",
      "HuggingFaceFW/finewiki\n",
      "Updated\n",
      "12 days ago\n",
      "‚Ä¢\n",
      "12.8k\n",
      "‚Ä¢\n",
      "200\n",
      "neulab/agent-data-collection\n",
      "Updated\n",
      "Sep 9\n",
      "‚Ä¢\n",
      "4.06k\n",
      "‚Ä¢\n",
      "52\n",
      "nvidia/Nemotron-VLM-Dataset-v2\n",
      "Updated\n",
      "4 days ago\n",
      "‚Ä¢\n",
      "1.7k\n",
      "‚Ä¢\n",
      "41\n",
      "fka/awesome-chatgpt-prompts\n",
      "Updated\n",
      "Jan 6\n",
      "‚Ä¢\n",
      "35.5k\n",
      "‚Ä¢\n",
      "9.33k\n",
      "Browse 250k+ datasets\n",
      "The Home of Machine Learning\n",
      "Create, discover and collaborate on ML better.\n",
      "The collaboration platform\n",
      "Host and collaborate on unlimited public models, datasets and applications.\n",
      "Move faster\n",
      "With the HF Open source stack.\n",
      "Explore all modalities\n",
      "Text, image, video, audio or even 3D.\n",
      "Build your portfolio\n",
      "Share your work with the world and build your ML profile.\n",
      "Sign Up\n",
      "Accelerate your ML\n",
      "We provide paid Compute and Enterprise solutions.\n",
      "Team & Enterprise\n",
      "Give your team the most advanced platform to build AI with enterpr\n",
      "\n",
      "### Link: Enterprise Solutions\n",
      "Enterprise Hub - Hugging Face\n",
      "\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Team & Enterprise Hub\n",
      "Scale your organization with the world‚Äôs leading AI platform\n",
      "Subscribe to\n",
      "Team\n",
      "starting at $20/user/month\n",
      "or\n",
      "Contact sales for\n",
      "Enterprise\n",
      "to explore flexible contract options\n",
      "Give your organization the most advanced platform to build AI with enterprise-grade security, access controls,\n",
      "\t\t\tdedicated support and more.\n",
      "Single Sign-On\n",
      "Connect securely to your identity provider with SSO integration.\n",
      "Regions\n",
      "Select, manage, and audit the location of your repository data.\n",
      "Audit Logs\n",
      "Stay in control with comprehensive logs that report on actions taken.\n",
      "Resource Groups\n",
      "Accurately manage access to repositories with granular access control.\n",
      "Token Management\n",
      "Centralized token control and custom approval policies for organization access.\n",
      "Analytics\n",
      "Track and analyze repository usage data in a single dashboard.\n",
      "Advanced Compute Options\n",
      "Increase scalability and performance with more compute options like ZeroGPU.\n",
      "ZeroGPU Quota Boost\n",
      "All organization members get 5x more ZeroGPU quota to get the most of Spaces.\n",
      "Private Datasets Viewer\n",
      "Enable the Dataset Viewer on your private datasets for easier collaboration.\n",
      "Private Storage\n",
      "Get an additional 1 TB of private storage for each member of your organization (then $25/month per extra TB).\n",
      "Inference Providers\n",
      "Enable organization billing for Inference Providers, monitor usage with analytics, and manage spending limits.\n",
      "Advanced security\n",
      "Configure organization-wide security policies and default repository visibility.\n",
      "Billing\n",
      "Control your budget effectively with managed billing and yearly commit options.\n",
      "Priority Support\n",
      "Maximize your platform usage with priority support from the Hugging Face team.\n",
      "Join the most forward-thinking AI organizations\n",
      "Everything you already know and love about Hugging Face in Enterprise mode.\n",
      "Subscribe to\n",
      "Team\n",
      "starting at $20/user/month\n",
      "or\n",
      "Contact sales for\n",
      "Enterprise\n",
      "to explore fl\n",
      "\n",
      "### Link: Pricing Information\n",
      "Hugging Face ‚Äì Pricing\n",
      "\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Pricing\n",
      "Leveling up AI collaboration and compute.\n",
      "Give your personal account or your organization the most advanced platform to build AI.\n",
      "PRO\n",
      "PRO Account\n",
      "Boost your personal HF experience\n",
      "Subscribe for\n",
      "$9\n",
      "per month\n",
      "Get PRO\n",
      "10√ó private storage capacity\n",
      "20√ó included inference credits\n",
      "8√ó ZeroGPU quota and highest queue priority\n",
      "Spaces Dev Mode & ZeroGPU Spaces hosting\n",
      "Publish blog articles on your HF profile\n",
      "Dataset Viewer for private datasets\n",
      "Show your support with a Pro badge\n",
      "Team\n",
      "Instant setup for growing teams\n",
      "Subscribe for\n",
      "$20\n",
      "per user per month\n",
      "Get Team (via credit card)\n",
      "SSO and SAML support\n",
      "Choose data location with Storage Regions\n",
      "Detailed action reviews with Audit Logs\n",
      "Granular access control via Resource Groups\n",
      "Repository usage Analytics\n",
      "Set auth policies and default repository visibility\n",
      "Centralized token control and approvals\n",
      "Dataset Viewer for private datasets\n",
      "Advanced compute options for Spaces\n",
      "All organization members get ZeroGPU and Inference Providers PRO benefits\n",
      "Enterprise\n",
      "Custom onboarding and enterprise features\n",
      "Starting at\n",
      "$50\n",
      "per user per month\n",
      "Contact Sales\n",
      "All benefits from the Team plan\n",
      "Highest storage, bandwidth, and API rate limits\n",
      "Managed billing with annual commitments\n",
      "Legal and Compliance processes\n",
      "Personalized support\n",
      "Need support to adopt the HF Hub in your organization? View our\n",
      "Expert Support\n",
      ".\n",
      "Hugging Face Hub\n",
      "free\n",
      "The HF Hub is the central place to explore, experiment, collaborate and build technology with Machine\n",
      "\t\t\t\t\tLearning.\n",
      "Join the open source Machine Learning movement!\n",
      "‚Üí\n",
      "Sign Up\n",
      "Create with ML\n",
      "Packed with ML features, like model eval, dataset viewer and much more.\n",
      "Collaborate\n",
      "Git based and designed for collaboration at its core.\n",
      "Play and learn\n",
      "Learn by experimenting and sharing with our awesome community.\n",
      "Build your ML portfolio\n",
      "Share your work with the world and build your own ML profile.\n",
      "Spaces Hardware\n",
      "Starting at $\n",
      "\n",
      "### Link: Careers Page\n",
      "Hugging Face - Current Openings\n",
      "\n",
      "\n",
      "\n",
      "### Link: Documentation Portal\n",
      "Hugging Face - Documentation\n",
      "\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Documentation\n",
      "Hub & Client Libraries\n",
      "Hub\n",
      "Host Git-based models, datasets, and Spaces on the HF Hub\n",
      "Hub Python Library\n",
      "Python client to interact with the Hugging Face Hub\n",
      "Huggingface.js\n",
      "JavaScript libraries for Hugging Face with built-in TS types\n",
      "Tasks\n",
      "Explore demos, models, and datasets for any ML tasks\n",
      "Dataset viewer\n",
      "API for metadata, stats, and content of HF Hub datasets\n",
      "Deployment & Inference\n",
      "Inference Providers\n",
      "Call 200k+ models hosted by our 10+ Inference partners\n",
      "Inference Endpoints (dedicated)\n",
      "Deploy models on dedicated & fully managed infrastructure on HF\n",
      "Deploying on AWS\n",
      "Train/deploy models from Hugging Face to AWS with DLCs\n",
      "Text Generation Inference\n",
      "Serve language models with TGI optimized toolkit\n",
      "Text Embeddings Inference\n",
      "Serve embeddings models with TEI optimized toolkit\n",
      "Microsoft Azure\n",
      "Deploy Hugging Face models on Microsoft Azure\n",
      "Core ML Libraries\n",
      "Transformers\n",
      "State-of-the-art AI models for PyTorch\n",
      "Diffusers\n",
      "State-of-the-art Diffusion models in PyTorch\n",
      "Datasets\n",
      "Access & share datasets for any ML tasks\n",
      "Transformers.js\n",
      "State-of-the-art ML running directly in your browser\n",
      "Tokenizers\n",
      "Fast tokenizers optimized for research & production\n",
      "Evaluate\n",
      "Evaluate and compare models performance\n",
      "timm\n",
      "State-of-the-art vision models: layers, optimizers, and utilities\n",
      "Sentence Transformers\n",
      "Embeddings, Retrieval, and Reranking\n",
      "Kernels\n",
      "Load and run compute kernels from the Hugging Face Hub\n",
      "Training & Optimization\n",
      "PEFT\n",
      "Parameter-efficient finetuning for large language models\n",
      "Accelerate\n",
      "Train PyTorch models with multi-GPU, TPU, mixed precision\n",
      "Optimum\n",
      "Optimize HF Transformers for faster training/inference\n",
      "AWS Trainium & Inferentia\n",
      "Train/deploy Transformers/Diffusers on AWS\n",
      "TRL\n",
      "Train transformers LMs with reinforcement learning\n",
      "Safetensors\n",
      "Safe way to store/distribute neural network weights\n",
      "Bitsandbytes\n",
      "Optimize and quantize models with bitsandbytes\n",
      "Lighteval\n",
      "All-\n",
      "\n",
      "### Link: Company Blog\n",
      "Hugging Face ‚Äì Blog\n",
      "\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Blog, Articles, and discussions\n",
      "New Article\n",
      "community\n",
      "guide\n",
      "open source collab\n",
      "partnerships\n",
      "research\n",
      "NLP\n",
      "Audio\n",
      "CV\n",
      "RL\n",
      "ethics\n",
      "Diffusion\n",
      "Game Development\n",
      "RLHF\n",
      "Leaderboard\n",
      "Case Studies\n",
      "LeRobot\n",
      "Inference Providers\n",
      "Community Articles\n",
      "view all\n",
      "Granite 4.0 Nano: Just how small can you go?\n",
      "By\n",
      "ibm-granite\n",
      "and 1 other\n",
      "‚Ä¢\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "96\n",
      "Why Did MiniMax M2 End Up as a Full Attention Model?\n",
      "By\n",
      "MiniMax-AI\n",
      "‚Ä¢\n",
      "4 days ago\n",
      "‚Ä¢\n",
      "38\n",
      "On the Shifting Global Compute Landscape\n",
      "By\n",
      "huggingface\n",
      "and 1 other\n",
      "‚Ä¢\n",
      "5 days ago\n",
      "‚Ä¢\n",
      "29\n",
      "The World‚Äôs First and Best Speed Painting Software\n",
      "By\n",
      "wang12390\n",
      "‚Ä¢\n",
      "5 days ago\n",
      "‚Ä¢\n",
      "27\n",
      "üõ°Ô∏è Nemotron PII: Synthesized Data for Privacy-Preserving AI\n",
      "By\n",
      "nvidia\n",
      "‚Ä¢\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "22\n",
      "What makes good reasoning data\n",
      "By\n",
      "MiniMax-AI\n",
      "‚Ä¢\n",
      "4 days ago\n",
      "‚Ä¢\n",
      "22\n",
      "Cosmos Predict 2.5 & Transfer 2.5: Evolving the World Foundation Models for Physical AI\n",
      "By\n",
      "nvidia\n",
      "‚Ä¢\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "17\n",
      "NVIDIA Isaac GR00T in LeRobot\n",
      "By\n",
      "nvidia\n",
      "and 4 others\n",
      "‚Ä¢\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "17\n",
      "Aligning to What? Rethinking Agent Generalization in MiniMax M2\n",
      "By\n",
      "MiniMax-AI\n",
      "‚Ä¢\n",
      "4 days ago\n",
      "‚Ä¢\n",
      "17\n",
      "How to Build a Healthcare Robot from Simulation to Deployment with NVIDIA Isaac for Healthcare\n",
      "By\n",
      "nvidia\n",
      "‚Ä¢\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "15\n",
      "NVIDIA Releases 8 Million Sample Open Dataset and Tooling for OCR, Image Reasoning, Image and Video QA Tasks\n",
      "By\n",
      "nvidia\n",
      "and 6 others\n",
      "‚Ä¢\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "14\n",
      "Can Your LLM Think Like a Professional? Introducing ProfBench\n",
      "By\n",
      "nvidia\n",
      "and 7 others\n",
      "‚Ä¢\n",
      "6 days ago\n",
      "‚Ä¢\n",
      "14\n",
      "LightOnOCR-1B: The Case for End-to-End and Efficient Domain-Specific Vision-Language Models for OCR\n",
      "By\n",
      "lightonai\n",
      "and 2 others\n",
      "‚Ä¢\n",
      "11 days ago\n",
      "‚Ä¢\n",
      "55\n",
      "3+ Years of ML & Society at Hugging Face ü§óü§ùüßë‚Äçü§ù‚Äçüßë\n",
      "By\n",
      "yjernite\n",
      "and 3 others\n",
      "‚Ä¢\n",
      "5 days ago\n",
      "‚Ä¢\n",
      "13\n",
      "Code a simple RAG from scratch\n",
      "By\n",
      "ngxson\n",
      "‚Ä¢\n",
      "Oct 29, 2024\n",
      "‚Ä¢\n",
      "238\n",
      "Advancing Predictive ADMET Modeling Through Community-Driven Science: The ExpansionRx-OpenADMET Blind Challenge\n",
      "By\n",
      "hugging-science\n",
      "and 1 other\n",
      "‚Ä¢\n",
      "7 days ago\n",
      "‚Ä¢\n",
      "10\n",
      "Projected Abliteration\n",
      "By\n",
      "grimjim\n",
      "‚Ä¢\n",
      "9 days a\n",
      "\n",
      "### Link: Learning Resources\n",
      "Hugging Face - Learn\n",
      "\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Learn\n",
      "LLM Course\n",
      "This course will teach you about large language models using libraries from the HF ecosystem\n",
      "Robotics Course\n",
      "This course will teach you to build robots with using LeRobot\n",
      "MCP Course\n",
      "This course will teach you about Model Context Protocol\n",
      "a smol course\n",
      "This smollest course on post-training AI models\n",
      "Agents Course\n",
      "Learn to build and deploy your own AI agents\n",
      "Deep RL Course\n",
      "This course will teach you about deep reinforcement learning using libraries from the HF ecosystem\n",
      "Community Computer Vision Course\n",
      "This course will teach you about computer vision ML using libraries and models from the HF ecosystem\n",
      "Audio Course\n",
      "Learn to apply transformers to audio data using libraries from the HF ecosystem\n",
      "Open-Source AI Cookbook\n",
      "A collection of open-source-powered notebooks by AI builders, for AI builders\n",
      "ML for Games Course\n",
      "This course will teach you about integrating AI models your game and using AI tools in your game development workflow\n",
      "Diffusion Course\n",
      "Learn about diffusion models & how to use them with diffusers\n",
      "ML for 3D Course\n",
      "Learn about 3D ML with libraries from the HF ecosystem\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "### Link: Brand Guidelines\n",
      "Brand assets - Hugging Face\n",
      "\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Hugging Face ¬∑ Brand assets\n",
      "HF Logos\n",
      ".svg\n",
      ".png\n",
      ".ai\n",
      ".svg\n",
      ".png\n",
      ".ai\n",
      ".svg\n",
      ".png\n",
      ".ai\n",
      "HF Colors\n",
      "#FFD21E\n",
      "#FF9D00\n",
      "#6B7280\n",
      "HF Bio\n",
      "Hugging Face is the collaboration platform for the machine learning community.\n",
      "\n",
      "The Hugging Face Hub works as a central place where anyone can share, explore, discover, and experiment with open-source ML. HF empowers the next generation of machine learning engineers, scientists, and end users to learn, collaborate and share their work to build an open and ethical AI future together.\n",
      "\n",
      "With the fast-growing community, some of the most used open-source ML libraries and tools, and a talented science team exploring the edge of tech, Hugging Face is at the heart of the AI revolution.\n",
      "Copy to clipboard\n",
      "HF Universe\n",
      "Find other assets available for use from the Hugging Face brand universe\n",
      "here\n",
      ".\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Changelog\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "### Link: Product Changelog\n",
      "Changelog - Hugging Face\n",
      "\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Changelog\n",
      "Keep track of latest changes on the Hugging Face Hub\n",
      "Oct 28, 25\n",
      "Upvote\n",
      "43\n",
      "+38\n",
      "Oct 28, 25\n",
      "Set Default Sorting in the Community Tab\n",
      "Upvote\n",
      "43\n",
      "+38\n",
      "Repository owners can now set the default sorting for their repository‚Äôs Discussions and Pull Requests (Community Tab) from the repository settings page. Choose between ‚ÄúTrending,‚Äù ‚ÄúMost Reactions,‚Äù or ‚ÄúRecently Created‚Äù to determine how discussions and contributions are sorted by default when visiting your repository‚Äôs Community Tab.\n",
      "Oct 23, 25\n",
      "Upvote\n",
      "64\n",
      "+59\n",
      "Oct 23, 25\n",
      "Cleaner Collection URLs\n",
      "Upvote\n",
      "64\n",
      "+59\n",
      "Collection pages now have shorter, cleaner links without the extra ID at the end. Old URLs will still work and automatically redirect, so your existing links remain valid.\n",
      "Oct 14, 25\n",
      "Upvote\n",
      "87\n",
      "+82\n",
      "Oct 14, 25\n",
      "Inference Providers Usage Breakdown\n",
      "Upvote\n",
      "87\n",
      "+82\n",
      "Users and organizations can view their usage of\n",
      "Inference Providers\n",
      "from their settings. Go to your\n",
      "Inference Providers Settings\n",
      "to view your usage for the past month, broken down per model and per provider.\n",
      "The same view is available for Organization subscribed to a paid plan under the organization's settings.\n",
      "Oct 9, 25\n",
      "Upvote\n",
      "61\n",
      "+56\n",
      "Oct 9, 25\n",
      "Organization tagging for Papers\n",
      "Upvote\n",
      "61\n",
      "+56\n",
      "Authors can now tag an Organization when submitting a paper. Each Organization has a dedicated Papers page that automatically lists its tagged publications. See examples:\n",
      "https://huggingface.co/nvidia/papers\n",
      "and\n",
      "https://huggingface.co/google/papers\n",
      ".\n",
      "This makes it easier for teams to showcase their research and for readers to discover work by lab, company, or community.\n",
      "Oct 7, 25\n",
      "Upvote\n",
      "76\n",
      "+71\n",
      "Oct 7, 25\n",
      "GGUF Metadata Editor\n",
      "Upvote\n",
      "76\n",
      "+71\n",
      "GGUF files up to 10 GB can now be edited directly on Hugging Face thanks to\n",
      "Xet\n",
      ". When opening a GGUF file, a new\n",
      "GGUF Editor\n",
      "label appears, enabling quick updates to metadata fields such as the chat template through the web interf\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "base_url = os.getenv(\"GEMINI_BASE_URL\")\n",
    "gemini_model = os.getenv(\"GEMINI_FM\")\n",
    "website_url = \"https://huggingface.co\"\n",
    "\n",
    "print(fetch_page_and_all_relevant_links_fmodel(api_key=api_key, base_url=base_url, model_name=gemini_model, website_url=website_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc2ce5",
   "metadata": {},
   "source": [
    "## Create system prompt for creating brochure from the content we fetched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc904c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "brochure_system_prompt = \"\"\"\n",
    "    You are an assistand thta analyses the contents of serveral relevant pages from a company website and \n",
    "    creates a short brochure about the company for prospective customers, investors and recruiters.\n",
    "    Respond in markdown format wihout code locks use horizontal rules (---) to separate sections.\n",
    "    Include details of company culture, customers and careers/jobs if you have that information.\n",
    "\"\"\"\n",
    "\n",
    "# Or uncomment the lines below for a more humorous brochure - this demonstrates how easy it is to incorporate 'tone':\n",
    "\n",
    "# brochure_system_prompt = \"\"\"\n",
    "# You are an assistant that analyzes the contents of several relevant pages from a company website\n",
    "# and creates a short, humorous, entertaining, witty brochure about the company for prospective customers, investors and recruits.\n",
    "# Respond in markdown without code blocks.\n",
    "# Include details of company culture, customers and careers/jobs if you have the information.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "795ae2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brochure_user_prompt_frontier_models(company_name, website_url):\n",
    "    user_prompt = f\"\"\"\n",
    "    You are looking at a comany called: {company_name}\n",
    "    Here's  the contents of it's landing page and other relevant pages;\n",
    "    use this infformation to build a short broucher of the company in markdown format without code blocks.\\n\\n\n",
    "    \"\"\"\n",
    "    user_prompt += fetch_page_and_all_relevant_links_fmodel(api_key=api_key, base_url=base_url, model_name=gemini_model, website_url=website_url)\n",
    "    user_prompt = user_prompt[:5_000]  # Truncate if more than 5000 characters\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dcee9bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gemini-2.5-flash\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n    You are looking at a comany called: Hugging Face\\n    Here's  the contents of it's landing page and other relevant pages;\\n    use this infformation to build a short broucher of the company in markdown format without code blocks.\\n\\n\\n    ## Landing Page: \\n\\nHugging Face ‚Äì The AI community building the future.\\n\\nHugging Face\\nModels\\nDatasets\\nSpaces\\nCommunity\\nDocs\\nEnterprise\\nPricing\\nLog In\\nSign Up\\nThe AI community building the future.\\nThe platform where the machine learning community collaborates on models, datasets, and applications.\\nExplore AI Apps\\nor\\nBrowse 1M+ models\\nTrending on\\nthis week\\nModels\\nMiniMaxAI/MiniMax-M2\\nUpdated\\n5 days ago\\n‚Ä¢\\n726k\\n‚Ä¢\\n979\\ndeepseek-ai/DeepSeek-OCR\\nUpdated\\n10 days ago\\n‚Ä¢\\n2.06M\\n‚Ä¢\\n2.41k\\nmoonshotai/Kimi-Linear-48B-A3B-Instruct\\nUpdated\\n2 days ago\\n‚Ä¢\\n15k\\n‚Ä¢\\n319\\nbriaai/FIBO\\nUpdated\\n1 day ago\\n‚Ä¢\\n2.84k\\n‚Ä¢\\n195\\ndx8152/Qwen-Edit-2509-Multiple-angles\\nUpdated\\nabout 9 hours ago\\n‚Ä¢\\n159\\nBrowse 1M+ models\\nSpaces\\nRunning\\non\\nCPU Upgrade\\n1.15k\\n1.15k\\nThe Smol Training Playbook: The Secrets to Building World-Class LLMs\\nüìù\\nRunning\\n15.6k\\n15.6k\\nDeepSite v3\\nüê≥\\nGenerate any application by Vibe Coding\\nRunning\\n2.25k\\n2.25k\\nWan2.2 Animate\\nüëÅ\\nWan2.2 Animate\\nRunning\\non\\nZero\\nMCP\\n118\\n118\\nDream-wan2-2-faster-Pro\\nüé¨\\nGenerate a video from an image with detailed prompts\\nRunning\\non\\nZero\\nMCP\\n2.03k\\n2.03k\\nWan2.2 14B Fast\\nüé•\\ngenerate a video from an image with a text prompt\\nBrowse 400k+ applications\\nDatasets\\nnvidia/PhysicalAI-Autonomous-Vehicles\\nUpdated\\n6 days ago\\n‚Ä¢\\n8.9k\\n‚Ä¢\\n157\\nHuggingFaceFW/finewiki\\nUpdated\\n12 days ago\\n‚Ä¢\\n12.8k\\n‚Ä¢\\n200\\nneulab/agent-data-collection\\nUpdated\\nSep 9\\n‚Ä¢\\n4.06k\\n‚Ä¢\\n52\\nnvidia/Nemotron-VLM-Dataset-v2\\nUpdated\\n4 days ago\\n‚Ä¢\\n1.7k\\n‚Ä¢\\n41\\nfka/awesome-chatgpt-prompts\\nUpdated\\nJan 6\\n‚Ä¢\\n35.5k\\n‚Ä¢\\n9.33k\\nBrowse 250k+ datasets\\nThe Home of Machine Learning\\nCreate, discover and collaborate on ML better.\\nThe collaboration platform\\nHost and collaborate on unlimited public models, datasets and applications.\\nMove faster\\nWith the HF Open source stack.\\nExplore all modalities\\nText, image, video, audio or even 3D.\\nBuild your portfolio\\nShare your work with the world and build your ML profile.\\nSign Up\\nAccelerate your ML\\nWe provide paid Compute and Enterprise solutions.\\nTeam & Enterprise\\nGive your team the most advanced platform to build AI with enterprise-grade\\n---\\n## Relevnant Links:\\n\\n\\n\\n### Link: homepage\\nHugging Face ‚Äì The AI community building the future.\\n\\nHugging Face\\nModels\\nDatasets\\nSpaces\\nCommunity\\nDocs\\nEnterprise\\nPricing\\nLog In\\nSign Up\\nThe AI community building the future.\\nThe platform where the machine learning community collaborates on models, datasets, and applications.\\nExplore AI Apps\\nor\\nBrowse 1M+ models\\nTrending on\\nthis week\\nModels\\nMiniMaxAI/MiniMax-M2\\nUpdated\\n5 days ago\\n‚Ä¢\\n726k\\n‚Ä¢\\n979\\ndeepseek-ai/DeepSeek-OCR\\nUpdated\\n10 days ago\\n‚Ä¢\\n2.06M\\n‚Ä¢\\n2.41k\\nmoonshotai/Kimi-Linear-48B-A3B-Instruct\\nUpdated\\n2 days ago\\n‚Ä¢\\n15k\\n‚Ä¢\\n319\\nbriaai/FIBO\\nUpdated\\n1 day ago\\n‚Ä¢\\n2.84k\\n‚Ä¢\\n195\\ndx8152/Qwen-Edit-2509-Multiple-angles\\nUpdated\\nabout 9 hours ago\\n‚Ä¢\\n159\\nBrowse 1M+ models\\nSpaces\\nRunning\\non\\nCPU Upgrade\\n1.15k\\n1.15k\\nThe Smol Training Playbook: The Secrets to Building World-Class LLMs\\nüìù\\nRunning\\n15.6k\\n15.6k\\nDeepSite v3\\nüê≥\\nGenerate any application by Vibe Coding\\nRunning\\n2.25k\\n2.25k\\nWan2.2 Animate\\nüëÅ\\nWan2.2 Animate\\nRunning\\non\\nZero\\nMCP\\n118\\n118\\nDream-wan2-2-faster-Pro\\nüé¨\\nGenerate a video from an image with detailed prompts\\nRunning\\non\\nZero\\nMCP\\n2.03k\\n2.03k\\nWan2.2 14B Fast\\nüé•\\ngenerate a video from an image with a text prompt\\nBrowse 400k+ applications\\nDatasets\\nnvidia/PhysicalAI-Autonomous-Vehicles\\nUpdated\\n6 days ago\\n‚Ä¢\\n8.9k\\n‚Ä¢\\n157\\nHuggingFaceFW/finewiki\\nUpdated\\n12 days ago\\n‚Ä¢\\n12.8k\\n‚Ä¢\\n200\\nneulab/agent-data-collection\\nUpdated\\nSep 9\\n‚Ä¢\\n4.06k\\n‚Ä¢\\n52\\nnvidia/Nemotron-VLM-Dataset-v2\\nUpdated\\n4 days ago\\n‚Ä¢\\n1.7k\\n‚Ä¢\\n41\\nfka/awesome-chatgpt-prompts\\nUpdated\\nJan 6\\n‚Ä¢\\n35.5k\\n‚Ä¢\\n9.33k\\nBrowse 250k+ datasets\\nThe Home of Machine Learning\\nCreate, discover and collaborate on ML better.\\nThe collaboration platform\\nHost and collaborate on unlimited public models, datasets and applications.\\nMove faster\\nWith the HF Open source stack.\\nExplore all modalities\\nText, image, video, audio or even 3D.\\nBuild your portfolio\\nShare your work with the world and build your ML profile.\\nSign Up\\nAccelerate your ML\\nWe provide paid Compute and Enterprise solutions.\\nTeam & Enterprise\\nGive your team the most advanced platform to build AI with enterprise-grade\\n\\n### Link: models page\\nModels ‚Äì Hugging Face\\n\\nHugging Face\\nModels\\nDatasets\\nSpaces\\nCommunity\\nDocs\\nEnterprise\\nPricing\\nLog In\\nSign Up\\nEdit Models filters\\nMain\\nTasks\\nLibraries\\nLanguages\\nLicenses\\nOther\\nTasks\\nText Generation\\nAny-to-Any\\nImage-Text-to-Text\\nImage-to-Text\\nImage-to-Image\\nText-to-Image\\nText-to-Video\\nText-to-Speech\\n+ 42\\nParameters\\nReset Parameters\\n< 1B\\n6B\\n12B\\n32B\\n128B\\n> 500B\\n< 1B\\n> 500B\\nLibraries\\nPyTorch\\ngoogle-tensorflow\\nTensorFlow\\nJAX\\nTransformers\\nDiffusers\\nSafetensors\\nONNX\\nGGUF\\nTransformers.js\\nMLX\\nKeras\\n+ 41\\nApps\\nvLLM\\nTGI\\nllama.cpp\\nMLX LM\\nLM Studio\\nOllama\\nJan\\n+ 13\\nInference Providers\\nGroq\\nNovita\\nNebius AI\\nCerebras\\nSambaNova\\nNscale\\nfal\\nHyperbolic\\n+ 10\\nApply filters\\nModels\\nFull-te\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_brochure_user_prompt_frontier_models(\"Hugging Face\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebccaf",
   "metadata": {},
   "source": [
    "Create brochure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f009a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brochure(company_name, website_url):\n",
    "    payload = [\n",
    "        {\"role\": \"system\", \"content\": brochure_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": get_brochure_user_prompt_frontier_models(company_name, website_url)}\n",
    "    ]\n",
    "\n",
    "    json_resp_format = {\"type\": \"text\"}\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    response = client.chat.completions.create(model=gemini_model, messages=payload, response_format=json_resp_format)\n",
    "    brochure = response.choices[0].message.content\n",
    "\n",
    "    display(Markdown(brochure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "544220de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gemini-2.5-flash\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hugging Face: The Home of Machine Learning - Building the Future of AI, Together\n",
       "\n",
       "Hugging Face is the leading platform and vibrant community where the world's machine learning experts, developers, and researchers collaborate on cutting-edge models, diverse datasets, and innovative applications. We are dedicated to accelerating the advancement of artificial intelligence by fostering an open and collaborative environment, truly being \"the AI community building the future.\"\n",
       "\n",
       "---\n",
       "\n",
       "**What We Offer**\n",
       "\n",
       "Hugging Face provides an unparalleled ecosystem designed to create, discover, and collaborate on ML better:\n",
       "\n",
       "*   **Models:** Explore and utilize over 1 million pre-trained models across various modalities, from advanced language models like MiniMax-M2 to OCR solutions like DeepSeek-OCR.\n",
       "*   **Datasets:** Access a vast collection of over 250,000 datasets, including specialized resources for autonomous vehicles and curated prompts, to train and fine-tune your ML projects.\n",
       "*   **Spaces (AI Applications):** Launch and experiment with over 400,000 interactive AI applications, or build your own. These range from tools for generating videos from images to advanced LLM training playbooks, runnable on various compute options.\n",
       "*   **Collaboration Platform:** Our platform facilitates seamless collaboration, allowing you to host and share an unlimited number of public models, datasets, and applications. Build your ML portfolio and connect with the global AI community.\n",
       "*   **Open Source Stack:** Leverage the powerful Hugging Face open-source stack to move faster and innovate with confidence.\n",
       "*   **Multi-Modality Support:** Work with all types of data ‚Äì text, image, video, audio, and even 3D.\n",
       "\n",
       "---\n",
       "\n",
       "**For Our Customers & Partners**\n",
       "\n",
       "Whether you are an individual researcher, a startup, or a large enterprise, Hugging Face offers solutions tailored to your needs:\n",
       "\n",
       "*   **Individuals & Teams:** Create, discover, and collaborate on ML projects. Our platform is a launchpad for your innovations, enabling you to share your work and build your machine learning profile within a thriving community.\n",
       "*   **Team & Enterprise Solutions:** Accelerate your organization's AI initiatives with enterprise-grade compute and platform solutions.\n",
       "    *   **Team Plans:** Starting at $20/user/month, offering advanced collaborative features.\n",
       "    *   **Enterprise Hub:** For larger organizations, we provide flexible contract options with paramount features like single sign-on (SSO), granular access controls, region selection for data residency, comprehensive audit logs, and dedicated support to ensure security and compliance at scale. Scale your organization with the world‚Äôs leading AI platform, giving your team the most advanced tools to build AI.\n",
       "\n",
       "---\n",
       "\n",
       "**Our Community & Culture**\n",
       "\n",
       "At the heart of Hugging Face is a vibrant, global community driven by the belief that collective intelligence and open collaboration are key to building the future of AI. Our culture fosters sharing, innovation, and continuous learning, providing a space where everyone can contribute to and benefit from the advancements in machine learning. We are genuinely the \"AI community building the future.\"\n",
       "\n",
       "---\n",
       "\n",
       "**Join Our Journey**\n",
       "\n",
       "While specific career opportunities are not detailed in this brochure, Hugging Face is continuously expanding and seeking passionate individuals to contribute to our mission. If you are enthusiastic about machine learning, open source, and building the future of AI, we encourage you to explore career opportunities directly on our website. Become a part of the team that empowers millions of developers and researchers worldwide.\n",
       "\n",
       "---\n",
       "\n",
       "**Connect With Hugging Face**\n",
       "\n",
       "Explore the future of AI with us. Discover models, datasets, and applications, or accelerate your team's ML development today.\n",
       "\n",
       "Visit HuggingFace.co to learn more, sign up, or contact our sales team for enterprise solutions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_brochure(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e304731",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Finally - A minor improvement\n",
    "\n",
    "With a small adjustment, we can change this so that the results stream back from OpenAI,\n",
    "with the familiar typewriter animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brochure_stream(company_name, website_url):\n",
    "    payload = [\n",
    "        {\"role\": \"system\", \"content\": brochure_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": get_brochure_user_prompt_frontier_models(company_name, website_url)}\n",
    "    ]\n",
    "\n",
    "    json_resp_format = {\"type\": \"text\"}\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    \n",
    "    stream = client.chat.completions.create(model=gemini_model, messages=payload, response_format=json_resp_format, stream=True)\n",
    "\n",
    "    \n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39b5c32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gemini-2.5-flash\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcreate_brochure_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHuggingFace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://huggingface.co\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mcreate_brochure_stream\u001b[39m\u001b[34m(company_name, website_url)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_brochure_stream\u001b[39m(company_name, website_url):\n\u001b[32m      2\u001b[39m     payload = [\n\u001b[32m      3\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: brochure_system_prompt},\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mget_brochure_user_prompt_frontier_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwebsite_url\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[32m      5\u001b[39m     ]\n\u001b[32m      7\u001b[39m     json_resp_format = {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m      8\u001b[39m     client = OpenAI(api_key=api_key, base_url=base_url)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mget_brochure_user_prompt_frontier_models\u001b[39m\u001b[34m(company_name, website_url)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_brochure_user_prompt_frontier_models\u001b[39m(company_name, website_url):\n\u001b[32m      2\u001b[39m     user_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33m    You are looking at a comany called: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_name\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33m    Here\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms  the contents of it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms landing page and other relevant pages;\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m    use this infformation to build a short broucher of the company in markdown format without code blocks.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     user_prompt += \u001b[43mfetch_page_and_all_relevant_links_fmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgemini_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwebsite_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwebsite_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     user_prompt = user_prompt[:\u001b[32m5_000\u001b[39m]  \u001b[38;5;66;03m# Truncate if more than 5000 characters\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m user_prompt\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mfetch_page_and_all_relevant_links_fmodel\u001b[39m\u001b[34m(api_key, base_url, model_name, website_url)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_page_and_all_relevant_links_fmodel\u001b[39m(api_key, base_url, model_name, website_url):\n\u001b[32m      3\u001b[39m     content = scraper.fetch_text_contents(website_url)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     relevant_links = \u001b[43mselect_relevant_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwebsite_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwebsite_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m#print(relevant_links)\u001b[39;00m\n\u001b[32m      7\u001b[39m     result = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m## Landing Page: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m## Relevnant Links:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mselect_relevant_links\u001b[39m\u001b[34m(base_url, model_name, api_key, website_url)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Instruct model to respond in JSON format\u001b[39;00m\n\u001b[32m     13\u001b[39m json_resp_format = {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mjson_object\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson_resp_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m result = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     18\u001b[39m links = json.loads(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/wrkspace/poc-lab/llm_engineering/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "create_brochure_stream(\"HuggingFace\", \"https://huggingface.co\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Eng (.venv)",
   "language": "python",
   "name": "llm-eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
